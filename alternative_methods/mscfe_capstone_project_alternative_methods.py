# -*- coding: utf-8 -*-
"""MscFE_Capstone_Project_alternative_methods

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jbka3dQGWjYKZuUFvNYSw5bC4wo4O7NT

# WorldQuant University
## MscFE690 Capstone Project
### Student Group 7119

Abhijeet Aanand

Amparo Garcia Garcia

*NOTE.- Plotly graphs are interactive but unfortunately for conversion to pdf they are not the best option and sometimes the graph will appear as a blank space, therefore there is also the correspondant variant with matplotlib and seaborn.*

# Packages needed
"""

!pip install riskfolio-lib

#Libraries we will need for the capstone project
import pandas as pd
import yfinance as yf
import numpy as np
import plotly.express as px
import seaborn as sn
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.offline as pyo
import plotly.io as pio
import cvxpy as cvx
import numpy as np
from scipy.optimize import nnls

pio.renderers.default = "notebook"

# List of some of the most important world stock indexes
#stock_indexes = pd.read_html('https://finance.yahoo.com/world-indices')[0]
#print('Shape of dataframe of stock indexes: ', stock_indexes.shape)
#Most traded stocks worldwide
#stock_indexes.sort_values(by="Volume", ascending=False).head()
#Most expensive indexes
#stock_indexes.sort_values(by="Last Price", ascending=False).head()

"""# World Wide Indexes"""

#Define some indexes from each continent
continents=['Africa', 'America', 'Asia', 'Europe', 'Oceania']
tickers_af=['^ZADOW', '^NQMAEURN'] #Dow Jones South Africa, Nasdaq Morocco EUR NTR
tickers_am=['^BVSP', '^DJI', '^IXIC', '^GSPC', '^MXX'] #Bovespa (Brasil), Dow Jones (USA), Nasdaq (USA), IPC Mexico
tickers_as=['^HSI', '^NSEI', '^N225'] #Hang Seng Index (China), Nifty 50 (India), Nikkei 255 (Japan)
tickers_eu=['^FCHI', '^GDAXI', '^IBEX', '^STOXX50E', '^FTSE'] #CAC 40 (France), DAX 30 (Germany), IBEX 35 (Spain), EuroStoxx 50 (Whole Euro Zone), FTSE 100 (UK)
tickers_oc=['^AXJO'] #S&P/ASX 200 (Australia)
start_date='2018-01-01'
end_date='2023-12-31'

def extract_data(tickers, start_date, end_date):
  #Inputs:
  # 1. tickers -> List of stocks tickers that are interesting for our index
  # 2. start_date -> Start date considered for the evolution of the index
  # 3. end_date -> Last date considered for the evolution of the index
  #Output:
  #It reuturns a list of three dataframes:
  # lista_df which is a list with all individual datasets for each stock
  # df_all which is an unique dataframe with one column for the adjusted price of each stock
  #df_returns_all which is an unique dataframe with one column for the returns of each stock
  df_all=pd.DataFrame()
  df_returns_all=pd.DataFrame()
  lista_df=[]
  for ticker in tickers:
    df=yf.download(ticker, start=start_date, end=end_date)
    df=df[['Adj Close', 'Volume']]
    lista_df.append(df)
    df_all[f'{ticker}']=df['Adj Close']
    df_returns_all[f'{ticker}']=np.log(df["Adj Close"] / df["Adj Close"].shift(1))

  return [lista_df, df_all, df_returns_all]

#Extract dataframes from different indices throughout the time
df_africa, df_af, df_ret_af=extract_data(tickers_af, start_date, end_date)
df_america, df_am, df_ret_am=extract_data(tickers_am, start_date, end_date)
df_asia, df_as, df_ret_as=extract_data(tickers_as, start_date, end_date)
df_europe, df_eu, df_ret_eu=extract_data(tickers_eu, start_date, end_date)
df_oceania, df_oc, df_ret_oc=extract_data(tickers_oc, start_date, end_date)
df_continents=[df_af, df_am, df_as, df_eu, df_oc]
df_returns_continents=[df_ret_af, df_ret_am, df_ret_as, df_ret_eu, df_ret_oc] #One dataframe as the join of all dataframes for different countries

#Plot the evolution of indexes thorughout the time regarding its continent
for i in range(len(continents)):
  print(f'Statistics for {continents[i]}')
  display(df_continents[i].describe())
  #display(px.line(df_continents[i], title=f'Stock Indexes from {continents[i]}'))
  (df_continents[i]).plot()
  plt.title(f'Stock Indexes from {continents[i]}')
  plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left')
  plt.show()

df_all_continents=pd.concat([df_af, df_am, df_as, df_eu, df_oc])
df_all_continents.head()

#px.line(df_all_continents, title=f'World Stock Indexes')
plt.figure(figsize=(100,10))
df_all_continents.plot()
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.title('World Stock Indexes')
plt.xlabel('Date')
plt.ylabel('Adj Close Price')
plt.show();

"""As it is shown in the graphs above, BOVESPA and NSEI are chosen because they are from BRICS countries so it will be interesting to analyze them regarding for example GDAXI (Germany) and IBEX (Spain). From USA Dow Jones (DJI) will be our choice.

From Africa and Australia none are chosen as there are not so many data available of them.

# GDAXI Index (Germany)
"""

# DAX 30 is the index of Germany. For this project it will be analyzed from 1st of January 2022 until 31st of December 2023.
start_date='2022-01-01'
end_date='2023-12-31'
gdaxi_stocks=['BAYN.DE' , 'VNA.DE' , 'ENR.DE' , 'SHL.DE' , 'RWE.DE' , 'SY1.DE' , '1COV.DE' , 'ADS.DE' , 'MRK.DE' , 'MTX.DE' , 'EOAN.DE' , 'BEI.DE' , 'DTE.DE' , 'HEI.DE' , 'ALV.DE' , 'P911.DE' , 'DB1.DE' , 'DHL.DE' , 'ZAL.DE' , 'FRE.DE' , 'DBK.DE' , 'HNR1.DE' , 'DTG.DE' , 'SIE.DE' , 'AIR.DE' , 'BMW.DE' , 'BAS.DE' , 'VOW3.DE' , 'CON.DE' , 'IFX.DE']

def get_correlations(market_index, continent, stocks, start_date, end_date):
  #Function that plots the correlation matrix of adjusted prices and returns of the stocks
  #Inputs:
  # 1. market_index -> stock of the index studied
  # 2. continent -> Continent of the country where the index belongs
  # 3. stocks -> List of stocks that are components of the index
  # 4. start_date -> Start date considered for the evolution of the index
  # 5. end_date -> Last date considered for the evolution of the index
  #Outputs:
  #1. Correlation matrix of the adjusted prices of stocks and their index
  #2. Correlation matrix of the returns of stocks and their index
  #Several plots of the correlation matrixes

  df1, df2, df3=extract_data(stocks, start_date, end_date);
  df2.fillna(0,inplace=True) #Adjusted close prices dataframe
  if continent=='EU':
    df2.loc[:,f'{market_index}']=df_eu[f'^{market_index}']
  elif continent=='AM':
    df2.loc[:,f'{market_index}']=df_am[f'^{market_index}']
  elif continent=='AF':
    df2.loc[:,f'{market_index}']=df_af[f'^{market_index}']
  elif continent=='AS':
    df2.loc[:,f'{market_index}']=df_as[f'^{market_index}']
  elif continent=='OC':
    df2.loc[:,f'{market_index}']=df_oc[f'^{market_index}']
  else:
    print('Not a valid continent')
  #display(px.line(df2.drop(f'{market_index}', axis=1), title=f'{market_index} market index components'))
  plt.figure(figsize=(100,10))
  (df2.drop(f'{market_index}', axis=1)).plot()
  plt.title(f'{market_index} market index components')
  plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
  plt.show()



  #display(px.line(df2[f'{market_index}'], title=f'{market_index} market index'))

  (df2[f'{market_index}']).plot()
  plt.title(f'{market_index} market index ')
  plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left')
  plt.show()


  #display(px.imshow(round(df2.corr(),3), text_auto=True, aspect="auto", title='Correlation of Adjusted Prices'))
  fig, ax = plt.subplots(figsize=(30, 15))

  sn.heatmap(round(df2.corr(),3), annot=True) #Plot correlation between adjusted close prices
  fig.set_tight_layout(True)
  plt.title('Correlation of Adjusted Prices')
  plt.show()
  #Now we will do the same with the returns
  df4 = df2.pct_change() #Returns dataframe
  df4 = df4[1:]
  #display(px.imshow(round(df4.corr(),3), text_auto=True, aspect="auto", title='Correlation of Returns'))

  fig, ax = plt.subplots(figsize=(30, 15))

  sn.heatmap(round(df4.corr(),3), annot=True) #Plot correlation between returns
  fig.set_tight_layout(True)
  plt.title('Correlation of Returns')
  plt.show()

  return [df2, df4]

df_gdaxi, df_gdaxi_returns= get_correlations('GDAXI', 'EU', gdaxi_stocks, start_date, end_date)

plt.figure(figsize=(12, 8))
for symbol in gdaxi_stocks:
    returns = df_gdaxi_returns[symbol].pct_change().dropna()
    sn.histplot(returns, bins=50, kde=True, label=symbol)
plt.title('Histogram of Daily Returns')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.xlabel('Daily Return')
plt.ylabel('Frequency')
plt.show()

"""## Replication methods

### 1st Method: Riskfolio-Lib

First, we tried an already programmed portfolio as the easiest and fastest way.
"""

import riskfolio as rp
import warnings
warnings.filterwarnings("ignore")

#Function to apply riskfolio with inputs of stocks, the index to replicate, the dates of start and ending
def get_data_rp(stocks, index, start_date, end_date):
  #Function that replicates the index with riskfolio library
  #Inputs:
  # 1. stocks -> Lists of stocks that are components of the index
  # 2. index -> Market index replicated
  # 3. start_date -> Start date considered for the evolution of the index
  # 4. end_date -> Last date considered for the evolution of the index
  #Outputs:
  #1. w : Vector of weights of each stock according the riskfolio
  #2. frontier: Frontier  of each stock in the replicated index according the riskfolio
  #3. port: Optimized portfolio
  stocks_dax=stocks
  stocks_dax.append(index)
  data = yf.download(stocks_dax, start=start_date, end=end_date)
  data = data.loc[:, ("Adj Close", slice(None))]
  data.columns = stocks_dax
  returns = data.pct_change()
  returns=returns[1:]
  returns.fillna(0, inplace=True)
  bench_returns = returns.pop(index).to_frame()
  port = rp.Portfolio(returns=returns)
  port.assets_stats(method_mu="hist", method_cov="hist")
  port.kindbench = False
  port.benchindex = bench_returns
  port.allowTE = True
  port.TE = 0.008
  w = port.optimization(model="Classic", rm="CVaR", obj="Sharpe", rf=0, l=0, hist=True)
  frontier = port.efficient_frontier(model="Classic", rm="CVaR",points=100,rf=0,hist=True)

  return [w, frontier, port]

gdaxi_stocks=['BAYN.DE' , 'VNA.DE' , 'ENR.DE' , 'SHL.DE' , 'RWE.DE' , 'SY1.DE' , '1COV.DE' , 'ADS.DE' , 'MRK.DE' , 'MTX.DE' , 'EOAN.DE' , 'BEI.DE' , 'DTE.DE' , 'HEI.DE' , 'ALV.DE' , 'P911.DE' , 'DB1.DE' , 'DHL.DE' , 'ZAL.DE' , 'FRE.DE' , 'DBK.DE' , 'HNR1.DE' , 'DTG.DE' , 'SIE.DE' , 'AIR.DE' , 'BMW.DE' , 'BAS.DE' , 'VOW3.DE' , 'CON.DE' , 'IFX.DE']
w, frontier, port=get_data_rp(gdaxi_stocks, '^GDAXI', start_date, end_date)

g1 = rp.plot_frontier(w_frontier=frontier,mu=port.mu,cov=port.cov,returns=port.returns,rm="CVaR",rf=0,cmap="viridis",w=w,label="Max Risk Adjusted Return Portfolio",marker="*")

g2=rp.plot_pie(w=w,title="Sharpe Mean CVaR",others=0.05,nrow=25,cmap="tab20",height=6,width=10);

df_gdaxi_rp=pd.DataFrame(df_gdaxi_returns.iloc[:, :-1].mul(list(w['weights']), axis=1).sum(axis=1))
df_gdaxi_rp.rename(columns={0:'GDAXI_Replicated_Riskfolio'}, inplace=True)
weights_riskfolio=list(w['weights'])
df_gdaxi_rp.head()

g3 = rp.plot_frontier_area(w_frontier=frontier, ax=None)

"""### 2nd Method: Give weights with Principal Component Analysis

Principal components is a method that is often used for calculating importances of some variables regarding the index
"""

gdaxi_stocks=['BAYN.DE', 'VNA.DE', 'ENR.DE', 'SHL.DE', 'RWE.DE', 'SY1.DE', '1COV.DE', 'ADS.DE', 'MRK.DE', 'MTX.DE', 'EOAN.DE', 'BEI.DE', 'DTE.DE', 'HEI.DE', 'ALV.DE', 'P911.DE', 'DB1.DE', 'DHL.DE', 'ZAL.DE', 'FRE.DE', 'DBK.DE', 'HNR1.DE', 'DTG.DE', 'SIE.DE', 'AIR.DE', 'BMW.DE', 'BAS.DE', 'VOW3.DE', 'CON.DE', 'IFX.DE', '^GDAXI']
num_stocks=len(gdaxi_stocks)
lista_df, df_gdaxi, df_gdaxi_returns=extract_data(gdaxi_stocks, start_date, end_date )
df_gdaxi_returns=df_gdaxi_returns.iloc[1:, :]
df_gdaxi_returns.fillna(0, inplace=True)
#px.box(df_gdaxi_returns)
plt.boxplot(df_gdaxi_returns)
plt.show()

"""There are some outliers in the stocks data."""

scaler=StandardScaler()
df_gdaxi_returns.iloc[:, :-1]=df_gdaxi_returns.iloc[:, :-1].replace(np.inf, np.nan)
df_gdaxi_returns.iloc[:, :-1]=df_gdaxi_returns.iloc[:, :-1].replace(-np.inf, np.nan)
df_gdaxi_returns.iloc[:, :-1]=df_gdaxi_returns.iloc[:, :-1].fillna(0)
pca = PCA(random_state=42)
pca.fit(df_gdaxi_returns.iloc[:, :-1])

# Calcular la varianza explicada y los pesos
variance = pca.explained_variance_ratio_
pesos = pca.components_.T @ variance
weights_pca=pesos
len(pesos)

df_gdaxi_returns_replicated=df_gdaxi_returns.copy()
for i in range(len(pesos)):
  df_gdaxi_returns_replicated.iloc[:, i]=df_gdaxi_returns_replicated.iloc[:, i]*pesos[i]
df_gdaxi_returns_replicated['GDAXI_REP_PCA']=df_gdaxi_returns_replicated.iloc[:,:-1].sum(axis=1)
df_gdaxi_returns_replicated=df_gdaxi_returns_replicated[['GDAXI_REP_PCA', '^GDAXI']]
df_gdaxi_returns_replicated.head()

#px.line(df_gdaxi_returns_replicated)
df_gdaxi_returns_replicated.plot()
plt.title("Replicated vs Original GDAXI returns")
plt.show();

pesos

important_pesos=[abs(x) for x in pesos if x>0.05]
important_labels=[stock for stock, x in zip(gdaxi_stocks, pesos) if x>0.05]
peso_resto=sum([abs(x) for x in pesos if x<0.05])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of GDAXI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 3rd Method: Sparse Method without MonteCarlo"""

#Function to replicate with sparse method without MonteCarlo
def sparse_tracking(num_stocks, returns, index_returns,UpperBound, threshold, r):
  #Function that replicates the market index with sparse method but without MonteCarlo
  #Inputs:
  #1. num_stocks: number of stocks considered in the index
  #2. returns: dataframe with the returns of stocks that are components of the index
  #3. index_returns: dataframe with the index returns
  #4. UpperBound: upperbound to force the method to converge
  #5. threshold: tolerance
  #6. r: risk free rate
  #Outputs:
  #1. opt_value: Optimized value for stocks in the replication solution
  #2. opt_w: Optimal weights for each stock in the replication solution
  #3. t: intermediate result to generate the tracking error
  #4. tracking_error: Tracking error of the solution
    beta = cvx.Variable(num_stocks)
    t = cvx.Variable()
    opt_sol = np.array([])
    opt_val = np.inf
    opt_t = np.inf
    tracking_error = cvx.sum_squares(cvx.matmul(returns,beta) - index_returns)
    aim = cvx.Minimize(tracking_error + t)
    bs_constr = [beta >= 0, t >= 0, cvx.sum(beta) == 1]

    for i in range(num_stocks):
        constr = bs_constr + [cvx.log(beta[i]) + cvx.log(t) >= cvx.log(r)]
        eq = cvx.Problem(aim, constr)
        eq.solve()
        upperBound = eq.value - t.value + r *(np.sum(np.array(beta.value).reshape(-1,) > threshold))
        if upperBound < UpperBound:
            UpperBound = upperBound
        if eq.value < opt_val:
            opt_val = eq.value
            opt_sol = beta.value
            opt_t = t.value
    print(f"Solution found at step {i}")
    opt_value = opt_val
    opt_w = np.array(opt_sol).reshape(-1,)
    t = opt_t
    tracking_error = opt_value - opt_t
    return [opt_value, opt_w, t, tracking_error]

gdaxi_stocks=['BAYN.DE', 'VNA.DE', 'ENR.DE', 'SHL.DE', 'RWE.DE', 'SY1.DE', '1COV.DE', 'ADS.DE', 'MRK.DE', 'MTX.DE', 'EOAN.DE', 'BEI.DE', 'DTE.DE', 'HEI.DE', 'ALV.DE', 'P911.DE', 'DB1.DE', 'DHL.DE', 'ZAL.DE', 'FRE.DE', 'DBK.DE', 'HNR1.DE', 'DTG.DE', 'SIE.DE', 'AIR.DE', 'BMW.DE', 'BAS.DE', 'VOW3.DE', 'CON.DE', 'IFX.DE']
num_stocks=len(gdaxi_stocks)
lista_df, df_gdaxi, df_gdaxi_returns=extract_data(gdaxi_stocks, start_date, end_date )
lista_df_index, df_gdaxi_index, df_gdaxi_index_returns=extract_data(['^GDAXI'], start_date, end_date )
df_gdaxi_index_returns=df_gdaxi_index_returns[1:]
df_gdaxi_index_returns.fillna(0, inplace=True)
df_gdaxi_returns=df_gdaxi_returns[1:]
df_gdaxi_returns.fillna(0, inplace=True)
index_returns=df_gdaxi.sum(axis=1)
index_returns=np.log(index_returns / index_returns.shift(1))
index_returns.dropna(inplace=True)
threshold= 1e-7
r=0.01
opt_value, opt_w, t, tracking_error= sparse_tracking(num_stocks, df_gdaxi_returns, df_gdaxi_index_returns['^GDAXI'],  np.inf, threshold, r)
stocks_considered=np.array(gdaxi_stocks)[opt_w > threshold]
print('Found ', len(stocks_considered), ' stocks with weight bigger than threshold: ', stocks_considered)
weights_sparse_withoutMC=opt_w
print("Optimal weights: " ,opt_w)
print("Tracking error: ", (tracking_error)**0.5 )
print("Optimal Solution ", opt_value)

df_gdaxi_sparse=pd.DataFrame(df_gdaxi_returns.mul(opt_w, axis=1).sum(axis=1))
df_gdaxi_sparse.rename(columns={0:'GDAXI_Replicated'}, inplace=True)
df_gdaxi_sparse['GDAXI_orig']=df_gdaxi_index_returns
df_gdaxi_sparse.head()

df_gdaxi_sparse.plot()
plt.title("Replicated vs Original GDAXI returns")
plt.show();

important_pesos=[abs(x) for x in opt_w if x>0.025]
important_labels=[stock for stock, x in zip(gdaxi_stocks, opt_w) if x>0.025]
peso_resto=sum([abs(x) for x in opt_w if x<0.025])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of GDAXI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 4th Method: Allocation using Non-Negative Least Squares (NNLS) Optimization

Method were weights are calculated by solving constrainted optimization problem where coefficients cannot be negative
"""

df_gdaxi.fillna(0, inplace=True)

#Aplication of nnls algorithm and plotting the main important stocks weights
result = nnls(df_gdaxi_returns, list(df_gdaxi_index_returns['^GDAXI']))
weights = result[0]
weights_nnls=weights
factor=result[1]
weights_plotting=[abs(x) for x in weights_nnls if x>0.025]
important_labels=[stock for stock, x in zip(gdaxi_stocks, weights_plotting) if x>0.025]
peso_resto=sum([abs(x) for x in weights_nnls if x<0.025])
weights_plotting.append(peso_resto)
important_labels.append('Others')
plt.pie(weights_plotting, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of GDAXI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

df_gdaxi_nnls=pd.DataFrame(df_gdaxi_returns.mul(weights, axis=1).sum(axis=1))
df_gdaxi_nnls.rename(columns={0:'GDAXI_NNLS'}, inplace=True)
df_gdaxi_nnls['GDAXI_orig']=df_gdaxi_index_returns
df_gdaxi_nnls.head()

"""## Comparison of several methods

### Histogram of daily returns
"""

#Creation of a common dataset to compare replication results
df_gdaxi_comp=df_gdaxi_sparse
df_gdaxi_comp['GDAXI_REP_PCA']=df_gdaxi_returns_replicated[['GDAXI_REP_PCA']]
df_gdaxi_comp['GDAXI_RISKFOLIO']=df_gdaxi_rp
df_gdaxi_comp['GDAXI_NNLS']=df_gdaxi_nnls[['GDAXI_NNLS']]

# Plot individual histograms for daily returns

for symbol in list(df_gdaxi_comp.columns):
    plt.figure(figsize=(6, 4))
    returns = df_gdaxi_comp[symbol].pct_change().dropna()
    sn.histplot(returns, bins=50, kde=True, label=symbol)
    plt.title('Histogram of Daily Returns')
    plt.xlabel('Daily Return')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

"""### Difference of returns



"""

#Let's calculate difference between them
df_gdaxi_comp['Error_SPARSE_WITHOUT_MC']=df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_Replicated']
df_gdaxi_comp['Error_PCA']=df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_REP_PCA']
df_gdaxi_comp['Error_RISKFOLIO']=df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_RISKFOLIO']
df_gdaxi_comp['Error_NNLS']=df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_NNLS']

df_gdaxi_comp[['Error_SPARSE_WITHOUT_MC', 'Error_PCA', 'Error_RISKFOLIO', 'Error_NNLS']].plot()
plt.title("Difference of returns method errors")
plt.show();

df_gdaxi_comp.mean()

"""### Tracking errors"""

df_tracking_error=pd.DataFrame()
df_tracking_error['SPARSE_WITHOUT_MC']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_Replicated']).apply(lambda x: np.sqrt(abs(x)/(df_gdaxi_comp['GDAXI_orig'].shape[0]-1)))
df_tracking_error['PCA']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_REP_PCA']).apply(lambda x: np.sqrt(abs(x)/(df_gdaxi_comp['GDAXI_orig'].shape[0]-1)))
df_tracking_error['RISKFOLIO']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_RISKFOLIO']).apply(lambda x: np.sqrt(abs(x)/(df_gdaxi_comp['GDAXI_orig'].shape[0]-1)))
df_tracking_error['NNLS']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_NNLS']).apply(lambda x: np.sqrt(abs(x)/(df_gdaxi_comp['GDAXI_orig'].shape[0]-1)))

df_tracking_error.head()

df_tracking_error.plot()
plt.title("Tracking errors")
plt.show();

df_tracking_error.mean()

df_tracking_error.describe()

"""### Tracking differences"""

df_tracking_difference=pd.DataFrame()
df_tracking_difference['SPARSE_WITHOUT_MC']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_Replicated'])
df_tracking_difference['PCA']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_REP_PCA'])
df_tracking_difference['RISKFOLIO']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_RISKFOLIO'])
df_tracking_difference['NNLS']=(df_gdaxi_comp['GDAXI_orig']-df_gdaxi_comp['GDAXI_NNLS'])
print(df_tracking_difference['SPARSE_WITHOUT_MC'].mean())
print(df_tracking_difference['PCA'].mean())
print(df_tracking_difference['RISKFOLIO'].mean())
print(df_tracking_difference['NNLS'].mean())

df_tracking_difference.plot()
plt.title("Tracking difference of different replication methods")
plt.show();

df_tracking_difference.mean()

df_gdaxi_comp.head()

"""### Total Performance"""

df_cummreturn=pd.DataFrame()
df_cummreturn['PCA']=(1 + df_gdaxi_comp['GDAXI_REP_PCA'].fillna(0)).cumprod() - 1
df_cummreturn['RISKFOLIO']=(1 + df_gdaxi_comp['GDAXI_RISKFOLIO'].fillna(0)).cumprod() - 1
df_cummreturn['SPARSE_WITHOUT_MC']=(1 + df_gdaxi_comp['GDAXI_Replicated'].fillna(0)).cumprod() - 1
df_cummreturn['NNLS']=(1 + df_gdaxi_comp['GDAXI_NNLS'].fillna(0)).cumprod() - 1
df_cummreturn['^GDAXI']=(1 + df_gdaxi_comp['GDAXI_orig'].fillna(0)).cumprod() - 1

df_cummreturn.plot()
plt.title("Total Perfomance: Cummulative returns DAX index")
plt.show();

df_cummreturn.mean()

"""### Sharpe index"""

# Suppose risk free rate for example 0.5%
risk_free_rate=0.5/100
excess_return=pd.DataFrame()
excess_return['^GDAXI']=df_gdaxi_comp['GDAXI_orig']-risk_free_rate
excess_return['PCA']=df_gdaxi_comp['GDAXI_REP_PCA']-risk_free_rate
excess_return['RISKFOLIO']=df_gdaxi_comp['GDAXI_RISKFOLIO']-risk_free_rate
excess_return['SPARSE_WITHOUT_MC']=df_gdaxi_comp['GDAXI_Replicated']-risk_free_rate
excess_return['NNLS']=df_gdaxi_comp['GDAXI_NNLS']-risk_free_rate
sharpe_ratio_orig=np.mean(excess_return['^GDAXI'])/np.std(excess_return['^GDAXI'])
sharpe_ratio_pca=np.mean(excess_return['PCA'])/np.std(excess_return['PCA'])
sharpe_ratio_riskfolio=np.mean(excess_return['RISKFOLIO'])/np.std(excess_return['RISKFOLIO'])
sharpe_ratio_sparse_without_mc=np.mean(excess_return['SPARSE_WITHOUT_MC'])/np.std(excess_return['SPARSE_WITHOUT_MC'])
sharpe_ratio_nnls=np.mean(excess_return['NNLS'])/np.std(excess_return['NNLS'])
print('Sharpe Ratios: ')
print('Original index: ', sharpe_ratio_orig)
print('Replication with PCA index: ', sharpe_ratio_pca)
print('Replication with Riskfolio index: ', sharpe_ratio_riskfolio)
print('Replication with Sparse without MonteCarlo index: ', sharpe_ratio_sparse_without_mc)
print('Replication with NNLS index: ', sharpe_ratio_nnls)
excess_return.plot()
plt.title("Excess of returns in replication DAX index")
plt.show();

"""### Comparison of prices: Costs



"""

df_prices=pd.DataFrame()
df_prices['SPARSE_WITHOUT_MC']=pd.DataFrame(df_gdaxi.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_prices['PCA']=pd.DataFrame(df_gdaxi.mul(weights_pca, axis=1).sum(axis=1))
df_prices['RISKFOLIO']=pd.DataFrame(df_gdaxi.mul(weights_riskfolio, axis=1).sum(axis=1))
df_prices['NNLS']=pd.DataFrame(df_gdaxi.mul(weights_nnls, axis=1).sum(axis=1))
df_prices['^GDAXI']=df_gdaxi_index

df_prices.head()

df_prices.describe()

df_prices[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Prices of replication DAX index")
plt.show();

df_prices.mean()

"""### Liquidity"""

def extract_liq(tickers, start_date, end_date):
  #Function to calculate the volumen of transactions and therefore gives an estimation of liquidity for each stock
  #Inputs:
  # 1. tickers -> List of tickers of stocks in the index
  # 2. start_date-> First date considered for the replication index
  # 3. end_date -> Last date considered for the replication index
  #Outputs:
  # 1. df_all -> dataframe where each column is the volume of transaction for that date for each stock
  df_all=pd.DataFrame()
  df_returns_all=pd.DataFrame()
  lista_df=[]
  for ticker in tickers:
    df=yf.download(ticker, start=start_date, end=end_date)
    df=df[['Adj Close', 'Volume']]
    df_all[f'{ticker}']=df['Volume']
  return df_all

gdaxi_stocks=['BAYN.DE', 'VNA.DE', 'ENR.DE', 'SHL.DE', 'RWE.DE', 'SY1.DE', '1COV.DE', 'ADS.DE', 'MRK.DE', 'MTX.DE', 'EOAN.DE', 'BEI.DE', 'DTE.DE', 'HEI.DE', 'ALV.DE', 'P911.DE', 'DB1.DE', 'DHL.DE', 'ZAL.DE', 'FRE.DE', 'DBK.DE', 'HNR1.DE', 'DTG.DE', 'SIE.DE', 'AIR.DE', 'BMW.DE', 'BAS.DE', 'VOW3.DE', 'CON.DE', 'IFX.DE']
df_volumen=extract_liq(gdaxi_stocks, start_date, end_date)
df_volumen_index=yf.download('^GDAXI', start=start_date, end=end_date)['Volume']
df_liq=pd.DataFrame()
df_liq['SPARSE_WITHOUT_MC']=pd.DataFrame(df_volumen.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_liq['PCA']=pd.DataFrame(df_volumen.mul(weights_pca, axis=1).sum(axis=1))
df_liq['RISKFOLIO']=pd.DataFrame(df_volumen.mul(weights_riskfolio, axis=1).sum(axis=1))
df_liq['NNLS']=pd.DataFrame(df_volumen.mul(weights_nnls, axis=1).sum(axis=1))
df_liq['^GDAXI']=df_volumen_index
df_liq.head()

df_liq[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Liquidity of DAX replicas")
plt.show();

df_liq.mean()

"""# IBEX 35 (SPAIN)

Let's apply the same method to different indexes: in this case IBEX 35
"""

ibex_stocks=['FER.MC' , 'AENA.MC' , 'IBE.MC' , 'MRL.MC' , 'IAG.MC' , 'NTGY.MC' , 'CLNX.MC' , 'TEF.MC' , 'ITX.MC' , 'ELE.MC' , 'LOG.MC' , 'RED.MC' , 'MTS.MC' , 'FDR.MC' , 'CABK.MC' , 'ANE.MC' , 'BKT.MC' , 'ANA.MC' , 'ENG.MC' , 'AMS.MC' , 'SAB.MC' , 'MAP.MC' , 'BBVA.MC' , 'COL.MC' , 'SAN.MC' , 'ACS.MC' , 'GRF.MC' , 'ACX.MC' , 'UNI.MC' ] #, 'PUIG.MC' was included in 2024
df_ibex, df_ibex_returns= get_correlations('IBEX', 'EU', ibex_stocks, start_date, end_date)
w, frontier, port=get_data_rp(ibex_stocks, '^IBEX', "2021-01-01", "2023-12-31")

"""## Replication methods

### 1st Method: Riskfolio-Lib
"""

g2=rp.plot_pie(w=w,title="Sharpe Mean CVaR",others=0.05,nrow=25,cmap="tab20",height=6,width=10);

g1 = rp.plot_frontier(w_frontier=frontier,mu=port.mu,cov=port.cov,returns=port.returns,rm="CVaR",rf=0,cmap="viridis",w=w,label="Max Risk Adjusted Return Portfolio",marker="*")

g3 = rp.plot_frontier_area(w_frontier=frontier, cmap="tab20", height=6, width=10, ax=None)

df_ibex_rp=pd.DataFrame(df_ibex_returns.iloc[:, :-1].mul(list(w['weights']), axis=1).sum(axis=1))
df_ibex_rp.rename(columns={0:'IBEX_Replicated_Riskfolio'}, inplace=True)
weights_riskfolio=list(w['weights'])
df_ibex_rp.head()

"""### 2nd Method: Give weights with Principal Component Analysis"""

ibex_stocks=['FER.MC' , 'AENA.MC' , 'IBE.MC' , 'MRL.MC' , 'IAG.MC' , 'NTGY.MC' , 'CLNX.MC' , 'TEF.MC' , 'ITX.MC' , 'ELE.MC' , 'LOG.MC' , 'RED.MC' , 'MTS.MC' , 'FDR.MC' , 'CABK.MC' , 'ANE.MC' , 'BKT.MC' , 'ANA.MC' , 'ENG.MC' , 'AMS.MC' , 'SAB.MC' , 'MAP.MC' , 'BBVA.MC' , 'COL.MC' , 'SAN.MC' , 'ACS.MC' , 'GRF.MC' , 'ACX.MC' , 'UNI.MC', '^IBEX' ] #, 'PUIG.MC' was included in 2024
num_stocks=len(ibex_stocks)
lista_df, df_ibex, df_ibex_returns=extract_data(ibex_stocks, start_date, end_date )
df_ibex_returns=df_ibex_returns.iloc[1:, :]
df_ibex_returns.fillna(0, inplace=True)
plt.boxplot(df_ibex_returns)
plt.show()

pesos

scaler=StandardScaler()
df_ibex_returns.iloc[:, :-1]=df_ibex_returns.iloc[:, :-1].replace(np.inf, np.nan)
df_ibex_returns.iloc[:, :-1]=df_ibex_returns.iloc[:, :-1].replace(-np.inf, np.nan)
df_ibex_returns.iloc[:, :-1]=df_ibex_returns.iloc[:, :-1].fillna(0)
pca = PCA(random_state=42)
pca.fit(df_ibex_returns.iloc[:, :-1])

# Calcular la varianza explicada y los pesos
variance = pca.explained_variance_ratio_
pesos = pca.components_.T @ variance
weights_pca=pesos
df_ibex_returns_replicated=df_ibex_returns.copy()
for i in range(len(pesos)):
  df_ibex_returns_replicated.iloc[:, i]=df_ibex_returns_replicated.iloc[:, i]*pesos[i]
df_ibex_returns_replicated['IBEX_REP_PCA']=df_ibex_returns_replicated.iloc[:,:-1].sum(axis=1)
df_ibex_returns_replicated=df_ibex_returns_replicated[['IBEX_REP_PCA', '^IBEX']]
df_ibex_returns_replicated.head()

df_ibex_returns_replicated.plot()
plt.title("Replicated vs Original IBEX returns")
plt.show();

important_pesos=[abs(x) for x in pesos if x>0.05]
important_labels=[stock for stock, x in zip(ibex_stocks, pesos) if x>0.05]
peso_resto=sum([abs(x) for x in pesos if x<0.05])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of IBEX stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 3rd Method: Sparse Method without MonteCarlo"""

ibex_stocks=['FER.MC' , 'AENA.MC' , 'IBE.MC' , 'MRL.MC' , 'IAG.MC' , 'NTGY.MC' , 'CLNX.MC' , 'TEF.MC' , 'ITX.MC' , 'ELE.MC' , 'LOG.MC' , 'RED.MC' , 'MTS.MC' , 'FDR.MC' , 'CABK.MC' , 'ANE.MC' , 'BKT.MC' , 'ANA.MC' , 'ENG.MC' , 'AMS.MC' , 'SAB.MC' , 'MAP.MC' , 'BBVA.MC' , 'COL.MC' , 'SAN.MC' , 'ACS.MC' , 'GRF.MC' , 'ACX.MC' , 'UNI.MC' ] #, 'PUIG.MC' was included in 2024
num_stocks=len(ibex_stocks)
lista_df, df_ibex, df_ibex_returns=extract_data(ibex_stocks, start_date, end_date )
lista_df_index, df_ibex_index, df_ibex_index_returns=extract_data(['^IBEX'], start_date, end_date )
df_ibex_index_returns=df_ibex_index_returns[1:]
df_ibex_index_returns.fillna(0, inplace=True)
df_ibex_returns=df_ibex_returns[1:]
df_ibex_returns.fillna(0, inplace=True)
index_returns=df_ibex.sum(axis=1)
index_returns=np.log(index_returns / index_returns.shift(1))
index_returns.dropna(inplace=True)
threshold= 1e-7
r=0.01
opt_value, opt_w, t, tracking_error= sparse_tracking(num_stocks, df_ibex_returns, df_ibex_index_returns['^IBEX'],  np.inf, threshold, r)
stocks_considered=np.array(ibex_stocks)[opt_w > threshold]
print('Found ', len(stocks_considered), ' stocks with weight bigger than threshold: ', stocks_considered)
weights_sparse_withoutMC=opt_w
print("Optimal weights: " ,opt_w)
print("Tracking error: ", (tracking_error)**0.5 )
print("Optimal Solution ", opt_value)

df_ibex_sparse=pd.DataFrame(df_ibex_returns.mul(opt_w, axis=1).sum(axis=1))
df_ibex_sparse.rename(columns={0:'IBEX_Replicated'}, inplace=True)
df_ibex_sparse['IBEX_orig']=df_ibex_index_returns
df_ibex_sparse.head()

df_ibex_sparse.plot()
plt.title("Replicated vs Original IBEX returns")
plt.show();

important_pesos=[abs(x) for x in opt_w if x>0.025]
important_labels=[stock for stock, x in zip(ibex_stocks, opt_w) if x>0.025]
peso_resto=sum([abs(x) for x in opt_w if x<0.025])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of IBEX stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 4th Method: Allocation using Non-Negative Least Squares (NNLS) Optimization


"""

df_ibex.fillna(0, inplace=True)
result = nnls(df_ibex_returns, list(df_ibex_index_returns['^IBEX']))
weights = result[0]
weights_nnls=weights
factor=result[1]
weights_plotting=[abs(x) for x in weights_nnls if x>0.025]
important_labels=[stock for stock, x in zip(ibex_stocks, weights_plotting) if x>0.025]
peso_resto=sum([abs(x) for x in weights_nnls if x<0.025])
weights_plotting.append(peso_resto)
important_labels.append('Others')
plt.pie(weights_plotting, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of ibex stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

df_ibex_nnls=pd.DataFrame(df_ibex_returns.mul(weights, axis=1).sum(axis=1))
df_ibex_nnls.rename(columns={0:'IBEX_NNLS'}, inplace=True)
df_ibex_nnls['IBEX_orig']=df_ibex_index_returns
df_ibex_nnls.head()

"""## Comparison of several methods

### Histogram of daily returns
"""

df_ibex_comp=df_ibex_sparse
df_ibex_comp['IBEX_REP_PCA']=df_ibex_returns_replicated[['IBEX_REP_PCA']]
df_ibex_comp['IBEX_RISKFOLIO']=df_ibex_rp
df_ibex_comp['IBEX_NNLS']=df_ibex_nnls[['IBEX_NNLS']]

# Plot individual histograms for daily returns

for symbol in list(df_ibex_comp.columns):
    plt.figure(figsize=(6, 4))
    returns = df_ibex_comp[symbol].pct_change().dropna()
    sn.histplot(returns, bins=50, kde=True, label=symbol)
    plt.title('Histogram of Daily Returns')
    plt.xlabel('Daily Return')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

"""### Difference of returns

"""

#Let's calculate difference between them
df_ibex_comp['Error_SPARSE_WITHOUT_MC']=df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_Replicated']
df_ibex_comp['Error_PCA']=df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_REP_PCA']
df_ibex_comp['Error_RISKFOLIO']=df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_RISKFOLIO']
df_ibex_comp['Error_NNLS']=df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_NNLS']

df_ibex_comp[['Error_SPARSE_WITHOUT_MC', 'Error_PCA', 'Error_RISKFOLIO', 'Error_NNLS']].plot()
plt.title("Replication method errors")
plt.show();

df_ibex_comp.mean()

"""### Tracking errors"""

df_tracking_error=pd.DataFrame()
df_tracking_error['SPARSE_WITHOUT_MC']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_Replicated']).apply(lambda x: np.sqrt(abs(x)/(df_ibex_comp['IBEX_orig'].shape[0]-1)))
df_tracking_error['PCA']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_REP_PCA']).apply(lambda x: np.sqrt(abs(x)/(df_ibex_comp['IBEX_orig'].shape[0]-1)))
df_tracking_error['RISKFOLIO']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_RISKFOLIO']).apply(lambda x: np.sqrt(abs(x)/(df_ibex_comp['IBEX_orig'].shape[0]-1)))
df_tracking_error['NNLS']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_NNLS']).apply(lambda x: np.sqrt(abs(x)/(df_ibex_comp['IBEX_orig'].shape[0]-1)))
df_tracking_error.plot()
plt.title("Tracking errors")
plt.show();

df_tracking_error.mean()

"""### Tracking differences"""

df_tracking_difference=pd.DataFrame()
df_tracking_difference['SPARSE_WITHOUT_MC']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_Replicated'])
df_tracking_difference['PCA']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_REP_PCA'])
df_tracking_difference['RISKFOLIO']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_RISKFOLIO'])
df_tracking_difference['NNLS']=(df_ibex_comp['IBEX_orig']-df_ibex_comp['IBEX_NNLS'])
print(df_tracking_difference['SPARSE_WITHOUT_MC'].mean())
print(df_tracking_difference['PCA'].mean())
print(df_tracking_difference['RISKFOLIO'].mean())
print(df_tracking_difference['NNLS'].mean())
df_tracking_difference.plot()
plt.title("Tracking difference of different replication methods")
plt.show();

df_tracking_difference.mean()

"""### Total Performance"""

df_cummreturn=pd.DataFrame()
df_cummreturn['PCA']=(1 + df_ibex_comp['IBEX_REP_PCA'].fillna(0)).cumprod() - 1
df_cummreturn['RISKFOLIO']=(1 + df_ibex_comp['IBEX_RISKFOLIO'].fillna(0)).cumprod() - 1
df_cummreturn['SPARSE_WITHOUT_MC']=(1 + df_ibex_comp['IBEX_Replicated'].fillna(0)).cumprod() - 1
df_cummreturn['NNLS']=(1 + df_ibex_comp['IBEX_NNLS'].fillna(0)).cumprod() - 1
df_cummreturn['^IBEX']=(1 + df_ibex_comp['IBEX_orig'].fillna(0)).cumprod() - 1
df_cummreturn.plot()
plt.title("Total Perfomance: Cummulative returns IBEX index")
plt.show();

df_cummreturn.mean()

"""### Sharpe index"""

# Suppose risk free rate for example 0.5%
risk_free_rate=0.5/100
excess_return=pd.DataFrame()
excess_return['^IBEX']=df_ibex_comp['IBEX_orig']-risk_free_rate
excess_return['PCA']=df_ibex_comp['IBEX_REP_PCA']-risk_free_rate
excess_return['RISKFOLIO']=df_ibex_comp['IBEX_RISKFOLIO']-risk_free_rate
excess_return['SPARSE_WITHOUT_MC']=df_ibex_comp['IBEX_Replicated']-risk_free_rate
excess_return['NNLS']=df_ibex_comp['IBEX_NNLS']-risk_free_rate
sharpe_ratio_orig=np.mean(excess_return['^IBEX'])/np.std(excess_return['^IBEX'])
sharpe_ratio_pca=np.mean(excess_return['PCA'])/np.std(excess_return['PCA'])
sharpe_ratio_riskfolio=np.mean(excess_return['RISKFOLIO'])/np.std(excess_return['RISKFOLIO'])
sharpe_ratio_sparse_without_mc=np.mean(excess_return['SPARSE_WITHOUT_MC'])/np.std(excess_return['SPARSE_WITHOUT_MC'])
sharpe_ratio_nnls=np.mean(excess_return['NNLS'])/np.std(excess_return['NNLS'])
print('Sharpe Ratios: ')
print('Original index: ', sharpe_ratio_orig)
print('Replication with PCA index: ', sharpe_ratio_pca)
print('Replication with Riskfolio index: ', sharpe_ratio_riskfolio)
print('Replication with Sparse without MonteCarlo index: ', sharpe_ratio_sparse_without_mc)
print('Replication with NNLS index: ', sharpe_ratio_nnls)
excess_return.plot()
plt.title("Excess of returns in replication IBEX index")
plt.show();



"""### Comparison of prices: Costs

"""

df_prices=pd.DataFrame()
df_prices['SPARSE_WITHOUT_MC']=pd.DataFrame(df_ibex.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_prices['PCA']=pd.DataFrame(df_ibex.mul(weights_pca, axis=1).sum(axis=1))
df_prices['RISKFOLIO']=pd.DataFrame(df_ibex.mul(weights_riskfolio, axis=1).sum(axis=1))
df_prices['NNLS']=pd.DataFrame(df_ibex.mul(weights_nnls, axis=1).sum(axis=1))
df_prices['^IBEX']=df_ibex_index

df_prices.head()

df_prices.describe()

df_prices[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Prices of replication IBEX index")
plt.show();

df_prices.mean()

"""### Liquidity"""

df_volumen=extract_liq(ibex_stocks, start_date, end_date)
df_volumen_index=yf.download('^IBEX', start=start_date, end=end_date)['Volume']
df_liq=pd.DataFrame()
df_liq['SPARSE_WITHOUT_MC']=pd.DataFrame(df_volumen.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_liq['PCA']=pd.DataFrame(df_volumen.mul(weights_pca, axis=1).sum(axis=1))
df_liq['RISKFOLIO']=pd.DataFrame(df_volumen.mul(weights_riskfolio, axis=1).sum(axis=1))
df_liq['NNLS']=pd.DataFrame(df_volumen.mul(weights_nnls, axis=1).sum(axis=1))
df_liq['^IBEX']=df_volumen_index
df_liq.head()

df_liq[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Liquidity of IBEX replicas")
plt.show();

df_liq.mean()

"""# BVSP Bovespa Index (Brasil)

Let's apply the same method to different indexes: in this case Boxespa Index from BRICS' country: Brasil
"""

bovespa_stocks=['EMBR3.SA' , 'CTIP3.SA' , 'ENBR3.SA' , 'CESP6.SA' , 'CIEL3.SA' , 'BRPR3.SA' , 'ABEV3.SA' , 'CPFE3.SA' , 'CMIG4.SA' , 'CSNA3.SA' , 'BBSE3.SA' , 'EQTL3.SA' , 'CYRE3.SA' , 'BRFS3.SA' , 'CPLE6.SA' , 'CCRO3.SA' , 'BBDC3.SA' , 'BRAP4.SA' , 'ELET3.SA' , 'BBAS3.SA' , 'BBDC4.SA' , 'CSAN3.SA' , 'ESTC3.SA' , 'ECOR3.SA' , 'BVMF3.SA' , '656690' , 'BRML3.SA' , 'GGBR4.SA' , 'BRKM5.SA' , 'FIBR3.SA' ]
df_bovespa, df_bov, df_ret_bov=extract_data(bovespa_stocks, start_date, end_date)
df_bov.head()

"""## Replication methods

### 1st Method: Riskfolio-Lib

We decided to exclude those tickers with NaN data in all rows:
'CTIP3.SA', 'ENBR3.SA', 'CESP6.SA', 'ESTC3.SA', 'BVMF3.SA', '656690', 'BRML3.SA', 'FIBR3.SA'
"""

bovespa_stocks=['EMBR3.SA' ,'CIEL3.SA' , 'BRPR3.SA' , 'ABEV3.SA' , 'CPFE3.SA' , 'CMIG4.SA' , 'CSNA3.SA' , 'BBSE3.SA' , 'EQTL3.SA' , 'CYRE3.SA' , 'BRFS3.SA' , 'CPLE6.SA' , 'CCRO3.SA' , 'BBDC3.SA' , 'BRAP4.SA' , 'ELET3.SA' , 'BBAS3.SA' , 'BBDC4.SA' , 'CSAN3.SA' ,  'ECOR3.SA' , 'GGBR4.SA' , 'BRKM5.SA'  ]
df_bvsp, df_bvsp_returns= get_correlations('BVSP', 'AM', bovespa_stocks, start_date, end_date)
w, frontier, port=get_data_rp(bovespa_stocks, '^BVSP', "2021-01-01", "2023-12-31")

g2=rp.plot_pie(w=w,title="Sharpe Mean CVaR",others=0.05,nrow=25,cmap="tab20",height=6,width=10);

g1 = rp.plot_frontier(w_frontier=frontier,mu=port.mu,cov=port.cov,returns=port.returns,rm="CVaR",rf=0,cmap="viridis",w=w,label="Max Risk Adjusted Return Portfolio",marker="*")

g3 = rp.plot_frontier_area(w_frontier=frontier, cmap="tab20", height=6, width=10, ax=None)

df_bvsp_rp=pd.DataFrame(df_bvsp_returns.iloc[:, :-1].mul(list(w['weights']), axis=1).sum(axis=1))
df_bvsp_rp.rename(columns={0:'BVSP_Replicated_Riskfolio'}, inplace=True)
weights_riskfolio=list(w['weights'])
df_bvsp_rp.head()

"""### 2nd Method: Give weights with Principal Component Analysis"""

bovespa_stocks=['EMBR3.SA' ,'CIEL3.SA' , 'BRPR3.SA' , 'ABEV3.SA' , 'CPFE3.SA' , 'CMIG4.SA' , 'CSNA3.SA' , 'BBSE3.SA' , 'EQTL3.SA' , 'CYRE3.SA' , 'BRFS3.SA' , 'CPLE6.SA' , 'CCRO3.SA' , 'BBDC3.SA' , 'BRAP4.SA' , 'ELET3.SA' , 'BBAS3.SA' , 'BBDC4.SA' , 'CSAN3.SA' ,  'ECOR3.SA' , 'GGBR4.SA' , 'BRKM5.SA', '^BVSP' ]
num_stocks=len(bovespa_stocks)


lista_df, df_bvsp, df_bvsp_returns=extract_data(bovespa_stocks, start_date, end_date )
df_bvsp_returns=df_bvsp_returns.iloc[1:, :]
df_bvsp_returns.fillna(0, inplace=True)
plt.boxplot(df_bvsp_returns)
plt.show()

scaler=StandardScaler()
df_bvsp_returns.iloc[:, :-1]=df_bvsp_returns.iloc[:, :-1].replace(np.inf, np.nan)
df_bvsp_returns.iloc[:, :-1]=df_bvsp_returns.iloc[:, :-1].replace(-np.inf, np.nan)
df_bvsp_returns.iloc[:, :-1]=df_bvsp_returns.iloc[:, :-1].fillna(0)
pca = PCA(random_state=42)
pca.fit(df_bvsp_returns.iloc[:, :-1])

# Calcular la varianza explicada y los pesos
variance = pca.explained_variance_ratio_
pesos = pca.components_.T @ variance
weights_pca=pesos
df_bvsp_returns_replicated=df_bvsp_returns.copy()
for i in range(len(pesos)):
  df_bvsp_returns_replicated.iloc[:, i]=df_bvsp_returns_replicated.iloc[:, i]*pesos[i]
df_bvsp_returns_replicated['BVSP_REP_PCA']=df_bvsp_returns_replicated.iloc[:,:-1].sum(axis=1)
df_bvsp_returns_replicated=df_bvsp_returns_replicated[['BVSP_REP_PCA', '^BVSP']]
df_bvsp_returns_replicated.head()

df_bvsp_returns_replicated.plot()
plt.title("Replicated vs Original BVSP returns")
plt.show();

important_pesos=[abs(x) for x in pesos if x>0.05]
important_labels=[stock for stock, x in zip(bovespa_stocks, pesos) if x>0.05]
peso_resto=sum([abs(x) for x in pesos if x<0.05])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of BVSP stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 3rd Method: Sparse Method without Montecarlo

"""

bovespa_stocks=['EMBR3.SA' ,'CIEL3.SA' , 'BRPR3.SA' , 'ABEV3.SA' , 'CPFE3.SA' , 'CMIG4.SA' , 'CSNA3.SA' , 'BBSE3.SA' , 'EQTL3.SA' , 'CYRE3.SA' , 'BRFS3.SA' , 'CPLE6.SA' , 'CCRO3.SA' , 'BBDC3.SA' , 'BRAP4.SA' , 'ELET3.SA' , 'BBAS3.SA' , 'BBDC4.SA' , 'CSAN3.SA' ,  'ECOR3.SA' , 'GGBR4.SA' , 'BRKM5.SA']
num_stocks=len(bovespa_stocks)
lista_df, df_bvsp, df_bvsp_returns=extract_data(bovespa_stocks, start_date, end_date )
lista_df_index, df_bvsp_index, df_bvsp_index_returns=extract_data(['^BVSP'], start_date, end_date )
df_bvsp_index_returns=df_bvsp_index_returns[1:]
df_bvsp_index_returns.fillna(0, inplace=True)
df_bvsp_returns=df_bvsp_returns[1:]
df_bvsp_returns.fillna(0, inplace=True)
index_returns=df_bvsp.sum(axis=1)
index_returns=np.log(index_returns / index_returns.shift(1))
index_returns.dropna(inplace=True)
threshold= 1e-7
r=0.01
opt_value, opt_w, t, tracking_error= sparse_tracking(num_stocks, df_bvsp_returns, df_bvsp_index_returns['^BVSP'],  np.inf, threshold, r)
stocks_considered=np.array(bovespa_stocks)[opt_w > threshold]
print('Found ', len(stocks_considered), ' stocks with weight bigger than threshold: ', stocks_considered)
weights_sparse_withoutMC=opt_w
print("Optimal weights: " ,opt_w)
print("Tracking error: ", (tracking_error)**0.5 )
print("Optimal Solution ", opt_value)

df_bvsp_sparse=pd.DataFrame(df_bvsp_returns.mul(opt_w, axis=1).sum(axis=1))
df_bvsp_sparse.rename(columns={0:'BVSP_Replicated'}, inplace=True)
df_bvsp_sparse['BVSP_orig']=df_bvsp_index_returns
df_bvsp_sparse.head()

df_bvsp_sparse.plot()
plt.title("Replicated vs Original BVSP returns")
plt.show();

important_pesos=[abs(x) for x in opt_w if x>0.025]
important_labels=[stock for stock, x in zip(bovespa_stocks, opt_w) if x>0.025]
peso_resto=sum([abs(x) for x in opt_w if x<0.025])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of BVSP stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 4th Method: Allocation using Non-Negative Least Squares (NNLS) Optimization

"""

df_bvsp.fillna(0, inplace=True)
result = nnls(df_bvsp_returns, list(df_bvsp_index_returns['^BVSP']))
weights = result[0]
weights_nnls=weights
factor=result[1]
weights_plotting=[abs(x) for x in weights_nnls if x>0.025]
important_labels=[stock for stock, x in zip(bovespa_stocks, weights_plotting) if x>0.025]
peso_resto=sum([abs(x) for x in weights_nnls if x<0.025])
weights_plotting.append(peso_resto)
important_labels.append('Others')
plt.pie(weights_plotting, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of BOVESPA stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

df_bvsp_nnls=pd.DataFrame(df_bvsp_returns.mul(weights, axis=1).sum(axis=1))
df_bvsp_nnls.rename(columns={0:'BVSP_NNLS'}, inplace=True)
df_bvsp_nnls['BVSP_orig']=df_bvsp_index_returns
df_bvsp_nnls.head()

"""## Comparison of several methods

### Histogram of daily returns
"""

df_bvsp_comp=df_bvsp_sparse
df_bvsp_comp['BVSP_REP_PCA']=df_bvsp_returns_replicated[['BVSP_REP_PCA']]
df_bvsp_comp['BVSP_RISKFOLIO']=df_bvsp_rp
df_bvsp_comp['BVSP_NNLS']=df_bvsp_nnls[['BVSP_NNLS']]

# Plot individual histograms for daily returns

for symbol in list(df_bvsp_comp.columns):
    plt.figure(figsize=(6, 4))
    returns = df_bvsp_comp[symbol].pct_change().dropna()
    sn.histplot(returns, bins=50, kde=True, label=symbol)
    plt.title('Histogram of Daily Returns')
    plt.xlabel('Daily Return')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

"""### Difference of returns

"""

#Let's calculate difference between them
df_bvsp_comp['Error_SPARSE_WITHOUT_MC']=df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_Replicated']
df_bvsp_comp['Error_PCA']=df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_REP_PCA']
df_bvsp_comp['Error_RISKFOLIO']=df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_RISKFOLIO']
df_bvsp_comp['Error_NNLS']=df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_NNLS']

df_bvsp_comp[['Error_SPARSE_WITHOUT_MC', 'Error_PCA', 'Error_RISKFOLIO', 'Error_NNLS']].plot()
plt.title("Replication method errors")
plt.show();

"""### Tracking errors

"""

df_tracking_error=pd.DataFrame()
df_tracking_error['SPARSE_WITHOUT_MC']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_Replicated']).apply(lambda x: np.sqrt(abs(x)/(df_bvsp_comp['BVSP_orig'].shape[0]-1)))
df_tracking_error['PCA']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_REP_PCA']).apply(lambda x: np.sqrt(abs(x)/(df_bvsp_comp['BVSP_orig'].shape[0]-1)))
df_tracking_error['RISKFOLIO']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_RISKFOLIO']).apply(lambda x: np.sqrt(abs(x)/(df_bvsp_comp['BVSP_orig'].shape[0]-1)))
df_tracking_error['NNLS']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_NNLS']).apply(lambda x: np.sqrt(abs(x)/(df_bvsp_comp['BVSP_orig'].shape[0]-1)))
df_tracking_error.plot()
plt.title("Tracking errors")
plt.show();

df_tracking_error.mean()

"""### Tracking differences

"""

df_tracking_difference=pd.DataFrame()
df_tracking_difference['SPARSE_WITHOUT_MC']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_Replicated'])
df_tracking_difference['PCA']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_REP_PCA'])
df_tracking_difference['RISKFOLIO']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_RISKFOLIO'])
df_tracking_difference['NNLS']=(df_bvsp_comp['BVSP_orig']-df_bvsp_comp['BVSP_NNLS'])
print(df_tracking_difference['SPARSE_WITHOUT_MC'].mean())
print(df_tracking_difference['PCA'].mean())
print(df_tracking_difference['RISKFOLIO'].mean())
print(df_tracking_difference['NNLS'].mean())
df_tracking_difference.plot()
plt.title("Tracking difference of different replication methods")
plt.show();

df_tracking_difference.mean()

"""### Total Performance"""

df_cummreturn=pd.DataFrame()
df_cummreturn['PCA']=(1 + df_bvsp_comp['BVSP_REP_PCA'].fillna(0)).cumprod() - 1
df_cummreturn['RISKFOLIO']=(1 + df_bvsp_comp['BVSP_RISKFOLIO'].fillna(0)).cumprod() - 1
df_cummreturn['SPARSE_WITHOUT_MC']=(1 + df_bvsp_comp['BVSP_Replicated'].fillna(0)).cumprod() - 1
df_cummreturn['NNLS']=(1 + df_bvsp_comp['BVSP_NNLS'].fillna(0)).cumprod() - 1
df_cummreturn['^BVSP']=(1 + df_bvsp_comp['BVSP_orig'].fillna(0)).cumprod() - 1
df_cummreturn.plot()
plt.title("Total Perfomance: Cummulative returns BVSP index")
plt.show();

df_cummreturn.mean()

"""### Sharpe index

"""

# Suppose risk free rate for example 0.5%
risk_free_rate=0.5/100
excess_return=pd.DataFrame()
excess_return['^BVSP']=df_bvsp_comp['BVSP_orig']-risk_free_rate
excess_return['PCA']=df_bvsp_comp['BVSP_REP_PCA']-risk_free_rate
excess_return['RISKFOLIO']=df_bvsp_comp['BVSP_RISKFOLIO']-risk_free_rate
excess_return['SPARSE_WITHOUT_MC']=df_bvsp_comp['BVSP_Replicated']-risk_free_rate
excess_return['NNLS']=df_bvsp_comp['BVSP_NNLS']-risk_free_rate
sharpe_ratio_orig=np.mean(excess_return['^BVSP'])/np.std(excess_return['^BVSP'])
sharpe_ratio_pca=np.mean(excess_return['PCA'])/np.std(excess_return['PCA'])
sharpe_ratio_riskfolio=np.mean(excess_return['RISKFOLIO'])/np.std(excess_return['RISKFOLIO'])
sharpe_ratio_sparse_without_mc=np.mean(excess_return['SPARSE_WITHOUT_MC'])/np.std(excess_return['SPARSE_WITHOUT_MC'])
sharpe_ratio_nnls=np.mean(excess_return['NNLS'])/np.std(excess_return['NNLS'])
print('Sharpe Ratios: ')
print('Original index: ', sharpe_ratio_orig)
print('Replication with PCA index: ', sharpe_ratio_pca)
print('Replication with Riskfolio index: ', sharpe_ratio_riskfolio)
print('Replication with Sparse without MonteCarlo index: ', sharpe_ratio_sparse_without_mc)
print('Replication with NNLS index: ', sharpe_ratio_nnls)
excess_return.plot()
plt.title("Excess of returns in replication BVSP index")
plt.show();

"""### Comparison of prices: Costs

"""

df_prices=pd.DataFrame()
df_prices['SPARSE_WITHOUT_MC']=pd.DataFrame(df_bvsp.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_prices['PCA']=pd.DataFrame(df_bvsp.mul(weights_pca, axis=1).sum(axis=1))
df_prices['RISKFOLIO']=pd.DataFrame(df_bvsp.mul(weights_riskfolio, axis=1).sum(axis=1))
df_prices['NNLS']=pd.DataFrame(df_bvsp.mul(weights_nnls, axis=1).sum(axis=1))
df_prices['^BVSP']=df_bvsp_index

df_prices.head()

df_prices.describe()

df_prices[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Prices of replication BVSP index")
plt.show();

df_prices.mean()

"""### Liquidity"""

df_volumen=extract_liq(bovespa_stocks, start_date, end_date)
df_volumen_index=yf.download('^BVSP', start=start_date, end=end_date)['Volume']
df_liq=pd.DataFrame()
df_liq['SPARSE_WITHOUT_MC']=pd.DataFrame(df_volumen.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_liq['PCA']=pd.DataFrame(df_volumen.mul(weights_pca, axis=1).sum(axis=1))
df_liq['RISKFOLIO']=pd.DataFrame(df_volumen.mul(weights_riskfolio, axis=1).sum(axis=1))
df_liq['NNLS']=pd.DataFrame(df_volumen.mul(weights_nnls, axis=1).sum(axis=1))
df_liq['^BVSP']=df_volumen_index
df_liq.head()

df_liq[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Liquidity of BVSP replicas")
plt.show();

df_liq.mean()

"""# DJI Dow Jones (USA)

Let's apply the same method to different indexes: in this case DJI (Dow Jones)

## Replication methods

### 1st Method: Riskfolio-Lib
"""

dji_stocks=['AAPL' , 'MSFT' , 'AMZN' , 'WMT' , 'JPM' , 'UNH' , 'V' , 'PG' , 'JNJ' , 'HD' , 'KO' , 'MRK' , 'CVX' , 'CRM' , 'MCD' , 'CSCO' , 'IBM' , 'AXP' , 'VZ' , 'AMGN' , 'GS' , 'CAT' , 'DIS' , 'HON' , 'NKE' , 'BA' , 'INTC' , 'MMM' , 'TRV' , 'DOW']

df_dji, df_dji_returns= get_correlations('DJI', 'AM', dji_stocks, start_date, end_date)
w, frontier, port=get_data_rp(dji_stocks, '^DJI', "2021-01-01", "2023-12-31")

g2=rp.plot_pie(w=w,title="Sharpe Mean CVaR",others=0.05,nrow=25,cmap="tab20",height=6,width=10);

g1 = rp.plot_frontier(w_frontier=frontier,mu=port.mu,cov=port.cov,returns=port.returns,rm="CVaR",rf=0,cmap="viridis",w=w,label="Max Risk Adjusted Return Portfolio",marker="*")

g3 = rp.plot_frontier_area(w_frontier=frontier, cmap="tab20", height=6, width=10, ax=None)

df_dji_rp=pd.DataFrame(df_dji_returns.iloc[:, :-1].mul(list(w['weights']), axis=1).sum(axis=1))
df_dji_rp.rename(columns={0:'DJI_Replicated_Riskfolio'}, inplace=True)
weights_riskfolio=list(w['weights'])
df_dji_rp.head()

"""### 2nd Method: Give weights with Principal Component Analysis

"""

dji_stocks=['AAPL' , 'MSFT' , 'AMZN' , 'WMT' , 'JPM' , 'UNH' , 'V' , 'PG' , 'JNJ' , 'HD' , 'KO' , 'MRK' , 'CVX' , 'CRM' , 'MCD' , 'CSCO' , 'IBM' , 'AXP' , 'VZ' , 'AMGN' , 'GS' , 'CAT' , 'DIS' , 'HON' , 'NKE' , 'BA' , 'INTC' , 'MMM' , 'TRV' , 'DOW', '^DJI']
num_stocks=len(dji_stocks)


lista_df, df_dji, df_dji_returns=extract_data(dji_stocks, start_date, end_date )
df_dji_returns=df_dji_returns.iloc[1:, :]
df_dji_returns.fillna(0, inplace=True)
plt.boxplot(df_dji_returns)
plt.show()

scaler=StandardScaler()
df_dji_returns.iloc[:, :-1]=df_dji_returns.iloc[:, :-1].replace(np.inf, np.nan)
df_dji_returns.iloc[:, :-1]=df_dji_returns.iloc[:, :-1].replace(-np.inf, np.nan)
df_dji_returns.iloc[:, :-1]=df_dji_returns.iloc[:, :-1].fillna(0)
pca = PCA(random_state=42)
pca.fit(df_dji_returns.iloc[:, :-1])

# Calcular la varianza explicada y los pesos
variance = pca.explained_variance_ratio_
pesos = pca.components_.T @ variance
weights_pca=pesos
df_dji_returns_replicated=df_dji_returns.copy()
for i in range(len(pesos)):
  df_dji_returns_replicated.iloc[:, i]=df_dji_returns_replicated.iloc[:, i]*pesos[i]
df_dji_returns_replicated['DJI_REP_PCA']=df_dji_returns_replicated.iloc[:,:-1].sum(axis=1)
df_dji_returns_replicated=df_dji_returns_replicated[['DJI_REP_PCA', '^DJI']]
df_dji_returns_replicated.head()

df_dji_returns_replicated.plot()
plt.title("Replicated vs Original DJI returns")
plt.show();

important_pesos=[abs(x) for x in pesos if x>0.05]
important_labels=[stock for stock, x in zip(dji_stocks, pesos) if x>0.05]
peso_resto=sum([abs(x) for x in pesos if x<0.05])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of DJI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 3rd Method: Sparse Method without Montecarlo

"""

dji_stocks=['AAPL' , 'MSFT' , 'AMZN' , 'WMT' , 'JPM' , 'UNH' , 'V' , 'PG' , 'JNJ' , 'HD' , 'KO' , 'MRK' , 'CVX' , 'CRM' , 'MCD' , 'CSCO' , 'IBM' , 'AXP' , 'VZ' , 'AMGN' , 'GS' , 'CAT' , 'DIS' , 'HON' , 'NKE' , 'BA' , 'INTC' , 'MMM' , 'TRV' , 'DOW']
num_stocks=len(dji_stocks)
lista_df, df_dji, df_dji_returns=extract_data(dji_stocks, start_date, end_date )
lista_df_index, df_dji_index, df_dji_index_returns=extract_data(['^DJI'], start_date, end_date )
df_dji_index_returns=df_dji_index_returns[1:]
df_dji_index_returns.fillna(0, inplace=True)
df_dji_returns=df_dji_returns[1:]
df_dji_returns.fillna(0, inplace=True)
index_returns=df_dji.sum(axis=1)
index_returns=np.log(index_returns / index_returns.shift(1))
index_returns.dropna(inplace=True)
threshold= 1e-7
r=0.01
opt_value, opt_w, t, tracking_error= sparse_tracking(num_stocks, df_dji_returns, df_dji_index_returns['^DJI'],  np.inf, threshold, r)
stocks_considered=np.array(dji_stocks)[opt_w > threshold]
print('Found ', len(stocks_considered), ' stocks with weight bigger than threshold: ', stocks_considered)
weights_sparse_withoutMC=opt_w
print("Optimal weights: " ,opt_w)
print("Tracking error: ", (tracking_error)**0.5 )
print("Optimal Solution ", opt_value)

df_dji_sparse=pd.DataFrame(df_dji_returns.mul(opt_w, axis=1).sum(axis=1))
df_dji_sparse.rename(columns={0:'DJI_Replicated'}, inplace=True)
df_dji_sparse['DJI_orig']=df_dji_index_returns
df_dji_sparse.head()

df_dji_sparse.plot()
plt.title("Replicated vs Original DJI returns")
plt.show();

important_pesos=[abs(x) for x in opt_w if x>0.025]
important_labels=[stock for stock, x in zip(dji_stocks, opt_w) if x>0.025]
peso_resto=sum([abs(x) for x in opt_w if x<0.025])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of DJI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 4th Method: Allocation using Non-Negative Least Squares (NNLS) Optimization

"""

df_dji.fillna(0, inplace=True)
result = nnls(df_dji_returns, list(df_dji_index_returns['^DJI']))
weights = result[0]
weights_nnls=weights
factor=result[1]
weights_plotting=[abs(x) for x in weights_nnls if x>0.025]
important_labels=[stock for stock, x in zip(dji_stocks, weights_plotting) if x>0.025]
peso_resto=sum([abs(x) for x in weights_nnls if x<0.025])
weights_plotting.append(peso_resto)
important_labels.append('Others')
plt.pie(weights_plotting, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of DJI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

df_dji_nnls=pd.DataFrame(df_dji_returns.mul(weights, axis=1).sum(axis=1))
df_dji_nnls.rename(columns={0:'DJI_NNLS'}, inplace=True)
df_dji_nnls['DJI_orig']=df_dji_index_returns
df_dji_nnls.head()

"""### 5th Sparse Index Replication with Sequential MonteCarlo

This method is created in a different notebook due to its complexity

### Allocation using Non-Negative Least Squares (NNLS) Optimization to compare with sparse index replication
"""

start_date1='2015-04-01'
end_date1='2020-01-31'
start_date2='2020-02-03'
end_date2='2024-09-30'


dji_stocks=['AAPL' , 'MSFT' , 'AMZN' , 'WMT' , 'JPM' , 'UNH' , 'V' , 'PG' , 'JNJ' , 'HD' , 'KO' , 'MRK' , 'CVX' , 'CRM' , 'MCD' , 'CSCO' , 'IBM' , 'AXP' , 'VZ' , 'AMGN' , 'GS' , 'CAT' , 'DIS' , 'HON' , 'NKE' , 'BA' , 'INTC' , 'MMM' , 'TRV' , 'DOW']
num_stocks=len(dji_stocks)
lista_df_pre, df_dji_pre, df_dji_returns_pre=extract_data(dji_stocks, start_date1, end_date1 )
lista_df_index_pre, df_dji_index_pre, df_dji_index_returns_pre=extract_data(['^DJI'], start_date1, end_date1 )
df_dji_index_returns_pre=df_dji_index_returns_pre[1:]
df_dji_index_returns_pre.fillna(0, inplace=True)
df_dji_returns_pre=df_dji_returns_pre[1:]
df_dji_returns_pre.fillna(0, inplace=True)
result = nnls(df_dji_returns_pre, list(df_dji_index_returns_pre['^DJI']))
weights_pre = result[0]
weights_nnls_pre=weights_pre
factor_pre=result[1]
weights_plotting=[abs(x) for x in weights_nnls_pre ]
important_labels=[stock for stock, x in zip(dji_stocks, weights_plotting) ]
peso_resto=sum([abs(x) for x in weights_pre if x<0])
weights_plotting.append(peso_resto)
important_labels.append('Others')
plt.pie(weights_plotting, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of DJI stocks PreCovid')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

plt.figure(figsize=(15,6))
plt.bar(x=important_labels, height=[x*100 for x in weights_plotting], color='lightblue')
plt.legend()
plt.xlabel('Category')
plt.title('Frequency of stocks selected in NNLS PreCovid')
plt.ylabel('Frequency')

plt.show()

dji_stocks=['AAPL' , 'MSFT' , 'AMZN' , 'WMT' , 'JPM' , 'UNH' , 'V' , 'PG' , 'JNJ' , 'HD' , 'KO' , 'MRK' , 'CVX' , 'CRM' , 'MCD' , 'CSCO' , 'IBM' , 'AXP' , 'VZ' , 'AMGN' , 'GS' , 'CAT' , 'DIS' , 'HON' , 'NKE' , 'BA' , 'INTC' , 'MMM' , 'TRV' , 'DOW']
num_stocks=len(dji_stocks)
lista_df_post, df_dji_post, df_dji_returns_post=extract_data(dji_stocks, start_date2, end_date2 )
lista_df_index_post, df_dji_index_post, df_dji_index_returns_post=extract_data(['^DJI'], start_date2, end_date2 )
df_dji_index_returns_post=df_dji_index_returns_post[1:]
df_dji_index_returns_post.fillna(0, inplace=True)
df_dji_returns_post=df_dji_returns_post[1:]
df_dji_returns_post.fillna(0, inplace=True)

result = nnls(df_dji_returns_post, list(df_dji_index_returns_post['^DJI']))
weights_post = result[0]
weights_nnls_post=weights_post
factor_post=result[1]
weights_plotting=[abs(x) for x in weights_nnls_post]
important_labels=[stock for stock, x in zip(dji_stocks, weights_plotting)]
peso_resto=sum([abs(x) for x in weights_plotting if x<0])
weights_plotting.append(peso_resto)
important_labels.append('Others')
plt.pie(weights_plotting, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of DJI stocks PostCovid')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

plt.figure(figsize=(15,6))

plt.bar(x=important_labels, height=[x*100 for x in weights_plotting], color='lightblue')
plt.legend()
plt.xlabel('Category')
plt.title('Frequency of stocks selected in NNLS PostCovid')
plt.ylabel('Frequency')

plt.show()

df_dji_nnls_pre=pd.DataFrame(df_dji_returns_pre.mul(weights_nnls_pre, axis=1).sum(axis=1))
df_dji_nnls_pre.rename(columns={0:'DJI_NNLS'}, inplace=True)
df_dji_nnls_pre['DJI_orig']=df_dji_index_returns_pre
df_dji_nnls_pre.head()

df_dji_nnls_post=pd.DataFrame(df_dji_returns_post.mul(weights_nnls_post, axis=1).sum(axis=1))
df_dji_nnls_post.rename(columns={0:'DJI_NNLS_POST'}, inplace=True)
df_dji_nnls_post['DJI_orig']=df_dji_index_returns_post
df_dji_nnls_post.head()

df_tracking_error_pre=(df_dji_nnls_post['DJI_orig']-df_dji_nnls_post['DJI_NNLS_POST']).apply(lambda x: np.sqrt(abs(x)/(df_dji_nnls_post['DJI_NNLS_POST'].shape[0]-1)))
df_tracking_error_post=(df_dji_nnls_pre['DJI_orig']-df_dji_nnls_pre['DJI_NNLS']).apply(lambda x: np.sqrt(abs(x)/(df_dji_nnls_pre['DJI_NNLS'].shape[0]-1)))
print('Tracking error pre: ', np.mean(df_tracking_error_pre))
print('Tracking error post: ', np.mean(df_tracking_error_post))

np.mean(df_tracking_error_pre)/0.00001

np.mean(np.mean(df_tracking_error_post))/0.00001

"""## Comparison of several methods

### Histogram of daily returns
"""

df_dji_comp=df_dji_sparse
df_dji_comp['DJI_REP_PCA']=df_dji_returns_replicated[['DJI_REP_PCA']]
df_dji_comp['DJI_RISKFOLIO']=df_dji_rp
df_dji_comp['DJI_NNLS']=df_dji_nnls[['DJI_NNLS']]

# Plot individual histograms for daily returns

for symbol in list(df_dji_comp.columns):
    plt.figure(figsize=(6, 4))
    returns = df_dji_comp[symbol].pct_change().dropna()
    sn.histplot(returns, bins=50, kde=True, label=symbol)
    plt.title('Histogram of Daily Returns')
    plt.xlabel('Daily Return')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

"""### Difference of returns

"""

#Let's calculate difference between them
df_dji_comp['Error_SPARSE_WITHOUT_MC']=df_dji_comp['DJI_orig']-df_dji_comp['DJI_Replicated']
df_dji_comp['Error_PCA']=df_dji_comp['DJI_orig']-df_dji_comp['DJI_REP_PCA']
df_dji_comp['Error_RISKFOLIO']=df_dji_comp['DJI_orig']-df_dji_comp['DJI_RISKFOLIO']
df_dji_comp['Error_NNLS']=df_dji_comp['DJI_orig']-df_dji_comp['DJI_NNLS']

df_dji_comp[['Error_SPARSE_WITHOUT_MC', 'Error_PCA', 'Error_RISKFOLIO', 'Error_NNLS']].plot()
plt.title("Replication method errors")
plt.show();

"""### Tracking errors

"""

df_tracking_error=pd.DataFrame()
df_tracking_error['SPARSE_WITHOUT_MC']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_Replicated']).apply(lambda x: np.sqrt(abs(x)/(df_dji_comp['DJI_orig'].shape[0]-1)))
df_tracking_error['PCA']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_REP_PCA']).apply(lambda x: np.sqrt(abs(x)/(df_dji_comp['DJI_orig'].shape[0]-1)))
df_tracking_error['RISKFOLIO']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_RISKFOLIO']).apply(lambda x: np.sqrt(abs(x)/(df_dji_comp['DJI_orig'].shape[0]-1)))
df_tracking_error['NNLS']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_NNLS']).apply(lambda x: np.sqrt(abs(x)/(df_dji_comp['DJI_orig'].shape[0]-1)))
df_tracking_error.plot()
plt.title("Tracking errors")
plt.show();

df_tracking_error.mean()

"""### Tracking differences

"""

df_tracking_difference=pd.DataFrame()
df_tracking_difference['SPARSE_WITHOUT_MC']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_Replicated'])
df_tracking_difference['PCA']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_REP_PCA'])
df_tracking_difference['RISKFOLIO']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_RISKFOLIO'])
df_tracking_difference['NNLS']=(df_dji_comp['DJI_orig']-df_dji_comp['DJI_NNLS'])
print(df_tracking_difference['SPARSE_WITHOUT_MC'].mean())
print(df_tracking_difference['PCA'].mean())
print(df_tracking_difference['RISKFOLIO'].mean())
print(df_tracking_difference['NNLS'].mean())
df_tracking_difference.plot()
plt.title("Tracking difference of different replication methods")
plt.show();

df_tracking_difference.mean()

"""### Total Performance

"""

df_cummreturn=pd.DataFrame()
df_cummreturn['PCA']=(1 + df_dji_comp['DJI_REP_PCA'].fillna(0)).cumprod() - 1
df_cummreturn['RISKFOLIO']=(1 + df_dji_comp['DJI_RISKFOLIO'].fillna(0)).cumprod() - 1
df_cummreturn['SPARSE_WITHOUT_MC']=(1 + df_dji_comp['DJI_Replicated'].fillna(0)).cumprod() - 1
df_cummreturn['NNLS']=(1 + df_dji_comp['DJI_NNLS'].fillna(0)).cumprod() - 1
df_cummreturn['^DJI']=(1 + df_dji_comp['DJI_orig'].fillna(0)).cumprod() - 1
df_cummreturn.plot()
plt.title("Total Perfomance: Cummulative returns DJI index")
plt.show();

df_cummreturn.mean()

"""### Sharpe index

"""

# Suppose risk free rate for example 0.5%
risk_free_rate=0.5/100
excess_return=pd.DataFrame()
excess_return['^DJI']=df_dji_comp['DJI_orig']-risk_free_rate
excess_return['PCA']=df_dji_comp['DJI_REP_PCA']-risk_free_rate
excess_return['RISKFOLIO']=df_dji_comp['DJI_RISKFOLIO']-risk_free_rate
excess_return['SPARSE_WITHOUT_MC']=df_dji_comp['DJI_Replicated']-risk_free_rate
excess_return['NNLS']=df_dji_comp['DJI_NNLS']-risk_free_rate
sharpe_ratio_orig=np.mean(excess_return['^DJI'])/np.std(excess_return['^DJI'])
sharpe_ratio_pca=np.mean(excess_return['PCA'])/np.std(excess_return['PCA'])
sharpe_ratio_riskfolio=np.mean(excess_return['RISKFOLIO'])/np.std(excess_return['RISKFOLIO'])
sharpe_ratio_sparse_without_mc=np.mean(excess_return['SPARSE_WITHOUT_MC'])/np.std(excess_return['SPARSE_WITHOUT_MC'])
sharpe_ratio_nnls=np.mean(excess_return['NNLS'])/np.std(excess_return['NNLS'])
print('Sharpe Ratios: ')
print('Original index: ', sharpe_ratio_orig)
print('Replication with PCA index: ', sharpe_ratio_pca)
print('Replication with Riskfolio index: ', sharpe_ratio_riskfolio)
print('Replication with Sparse without MonteCarlo index: ', sharpe_ratio_sparse_without_mc)
print('Replication with NNLS index: ', sharpe_ratio_nnls)
excess_return.plot()
plt.title("Excess of returns in replication DJI index")
plt.show();

"""### Comparison of prices: Costs

"""

df_prices=pd.DataFrame()
df_prices['SPARSE_WITHOUT_MC']=pd.DataFrame(df_dji.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_prices['PCA']=pd.DataFrame(df_dji.mul(weights_pca, axis=1).sum(axis=1))
df_prices['RISKFOLIO']=pd.DataFrame(df_dji.mul(weights_riskfolio, axis=1).sum(axis=1))
df_prices['NNLS']=pd.DataFrame(df_dji.mul(weights_nnls, axis=1).sum(axis=1))
df_prices['^DJI']=df_dji_index

df_prices.head()

df_prices.describe()

df_prices[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Prices of replication DJI index")
plt.show();

df_prices.mean()

"""### Liquidity

"""

df_volumen=extract_liq(dji_stocks, start_date, end_date)
df_volumen_index=yf.download('^DJI', start=start_date, end=end_date)['Volume']
df_liq=pd.DataFrame()
df_liq['SPARSE_WITHOUT_MC']=pd.DataFrame(df_volumen.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_liq['PCA']=pd.DataFrame(df_volumen.mul(weights_pca, axis=1).sum(axis=1))
df_liq['RISKFOLIO']=pd.DataFrame(df_volumen.mul(weights_riskfolio, axis=1).sum(axis=1))
df_liq['NNLS']=pd.DataFrame(df_volumen.mul(weights_nnls, axis=1).sum(axis=1))
df_liq['^DJI']=df_volumen_index
df_liq.head()

df_liq[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Liquidity of DJI replicas")
plt.show();

df_liq.mean()

"""# NIFTY 50 (NSEI) (INDIA)

Let's apply the same method to different indexes: in this case NIFTY 50 from another BRICS' country India

## Replication methods

### 1st Method: Riskfolio-Lib
"""

nsei_stocks=['ADANIENT.NS' , 'ADANIPORTS.NS' , 'APOLLOHOSP.NS' , 'ASIANPAINT.NS' , 'AXISBANK.NS' , 'BAJAJ-AUTO.NS' , 'BAJFINANCE.NS' , 'BAJAJFINSV.NS' , 'BHARTIARTL.NS' , 'BPCL.NS' , 'CIPLA.NS' , 'COALINDIA.NS' , 'DIVISLAB.NS' , 'DRREDDY.NS' , 'EICHERMOT.NS' , 'GRASIM.NS' , 'HCLTECH.NS' , 'HDFCBANK.NS' , 'HDFCLIFE.NS' , 'HEROMOTOCO.NS' , 'HINDALCO.NS' , 'HINDUNILVR.NS' , 'ICICIBANK.NS' , 'INDUSINDBK.NS' , 'INFY.NS' , 'ITC.NS' , 'JSWSTEEL.NS' , 'KOTAKBANK.NS' , 'LT.NS' , 'M&M.NS' , 'MARUTI.NS' , 'NESTLEIND.NS' , 'NTPC.NS' , 'ONGC.NS' , 'POWERGRID.NS' , 'RELIANCE.NS' , 'SBIN.NS' , 'SBILIFE.NS' , 'SHRIRAMFIN.NS' , 'SUNPHARMA.NS' , 'TATACONSUM.NS' , 'TATAMOTORS.NS' , 'TATASTEEL.NS' , 'TCS.NS' , 'TECHM.NS' , 'TITAN.NS' , 'ULTRACEMCO.NS' , 'WIPRO.NS' , 'BRITANNIA.NS' , 'UPL.NS']
df_nsei, df_nsei_returns= get_correlations('NSEI', 'AS', nsei_stocks, start_date, end_date)
w, frontier, port=get_data_rp(nsei_stocks, '^NSEI', "2021-01-01", "2023-12-31")
df_nsei.head()

g2=rp.plot_pie(w=w,title="Sharpe Mean CVaR",others=0.05,nrow=25,cmap="tab20",height=6,width=10);

g1 = rp.plot_frontier(w_frontier=frontier,mu=port.mu,cov=port.cov,returns=port.returns,rm="CVaR",rf=0,cmap="viridis",w=w,label="Max Risk Adjusted Return Portfolio",marker="*")

g3 = rp.plot_frontier_area(w_frontier=frontier, cmap="tab20", height=6, width=10, ax=None)

df_nsei_rp=pd.DataFrame(df_nsei_returns.iloc[:, :-1].mul(list(w['weights']), axis=1).sum(axis=1))
df_nsei_rp.rename(columns={0:'NSEI_Replicated_Riskfolio'}, inplace=True)
weights_riskfolio=list(w['weights'])
df_nsei_rp.head()

"""### 2nd Method: Give weights with Principal Component Analysis

"""

nsei_stocks=['ADANIENT.NS' , 'ADANIPORTS.NS' , 'APOLLOHOSP.NS' , 'ASIANPAINT.NS' , 'AXISBANK.NS' , 'BAJAJ-AUTO.NS' , 'BAJFINANCE.NS' , 'BAJAJFINSV.NS' , 'BHARTIARTL.NS' , 'BPCL.NS' , 'CIPLA.NS' , 'COALINDIA.NS' , 'DIVISLAB.NS' , 'DRREDDY.NS' , 'EICHERMOT.NS' , 'GRASIM.NS' , 'HCLTECH.NS' , 'HDFCBANK.NS' , 'HDFCLIFE.NS' , 'HEROMOTOCO.NS' , 'HINDALCO.NS' , 'HINDUNILVR.NS' , 'ICICIBANK.NS' , 'INDUSINDBK.NS' , 'INFY.NS' , 'ITC.NS' , 'JSWSTEEL.NS' , 'KOTAKBANK.NS' , 'LT.NS' , 'M&M.NS' , 'MARUTI.NS' , 'NESTLEIND.NS' , 'NTPC.NS' , 'ONGC.NS' , 'POWERGRID.NS' , 'RELIANCE.NS' , 'SBIN.NS' , 'SBILIFE.NS' , 'SHRIRAMFIN.NS' , 'SUNPHARMA.NS' , 'TATACONSUM.NS' , 'TATAMOTORS.NS' , 'TATASTEEL.NS' , 'TCS.NS' , 'TECHM.NS' , 'TITAN.NS' , 'ULTRACEMCO.NS' , 'WIPRO.NS' , 'BRITANNIA.NS' , 'UPL.NS', '^NSEI']
num_stocks=len(nsei_stocks)


lista_df, df_nsei, df_nsei_returns=extract_data(nsei_stocks, start_date, end_date )
df_nsei_returns=df_nsei_returns.iloc[1:, :]
df_nsei_returns.fillna(0, inplace=True)
plt.boxplot(df_nsei_returns)
plt.show()

scaler=StandardScaler()
df_nsei_returns.iloc[:, :-1]=df_nsei_returns.iloc[:, :-1].replace(np.inf, np.nan)
df_nsei_returns.iloc[:, :-1]=df_nsei_returns.iloc[:, :-1].replace(-np.inf, np.nan)
df_nsei_returns.iloc[:, :-1]=df_nsei_returns.iloc[:, :-1].fillna(0)
pca = PCA(random_state=42)
pca.fit(df_nsei_returns.iloc[:, :-1])

# Calcular la varianza explicada y los pesos
variance = pca.explained_variance_ratio_
pesos = pca.components_.T @ variance
weights_pca=pesos
df_nsei_returns_replicated=df_nsei_returns.copy()
for i in range(len(pesos)):
  df_nsei_returns_replicated.iloc[:, i]=df_nsei_returns_replicated.iloc[:, i]*pesos[i]
df_nsei_returns_replicated['NSEI_REP_PCA']=df_nsei_returns_replicated.iloc[:,:-1].sum(axis=1)
df_nsei_returns_replicated=df_nsei_returns_replicated[['NSEI_REP_PCA', '^NSEI']]
df_nsei_returns_replicated.head()

df_nsei_returns_replicated.plot()
plt.title("Replicated vs Original DJI returns")
plt.show();

important_pesos=[abs(x) for x in pesos if x>0.05]
important_labels=[stock for stock, x in zip(nsei_stocks, pesos) if x>0.05]
peso_resto=sum([abs(x) for x in pesos if x<0.05])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of NSEI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 3rd Method: Sparse Method without Montecarlo

"""

nsei_stocks=['ADANIENT.NS' , 'ADANIPORTS.NS' , 'APOLLOHOSP.NS' , 'ASIANPAINT.NS' , 'AXISBANK.NS' , 'BAJAJ-AUTO.NS' , 'BAJFINANCE.NS' , 'BAJAJFINSV.NS' , 'BHARTIARTL.NS' , 'BPCL.NS' , 'CIPLA.NS' , 'COALINDIA.NS' , 'DIVISLAB.NS' , 'DRREDDY.NS' , 'EICHERMOT.NS' , 'GRASIM.NS' , 'HCLTECH.NS' , 'HDFCBANK.NS' , 'HDFCLIFE.NS' , 'HEROMOTOCO.NS' , 'HINDALCO.NS' , 'HINDUNILVR.NS' , 'ICICIBANK.NS' , 'INDUSINDBK.NS' , 'INFY.NS' , 'ITC.NS' , 'JSWSTEEL.NS' , 'KOTAKBANK.NS' , 'LT.NS' , 'M&M.NS' , 'MARUTI.NS' , 'NESTLEIND.NS' , 'NTPC.NS' , 'ONGC.NS' , 'POWERGRID.NS' , 'RELIANCE.NS' , 'SBIN.NS' , 'SBILIFE.NS' , 'SHRIRAMFIN.NS' , 'SUNPHARMA.NS' , 'TATACONSUM.NS' , 'TATAMOTORS.NS' , 'TATASTEEL.NS' , 'TCS.NS' , 'TECHM.NS' , 'TITAN.NS' , 'ULTRACEMCO.NS' , 'WIPRO.NS' , 'BRITANNIA.NS' , 'UPL.NS']
num_stocks=len(nsei_stocks)
lista_df, df_nsei, df_nsei_returns=extract_data(nsei_stocks, start_date, end_date )
lista_df_index, df_nsei_index, df_nsei_index_returns=extract_data(['^NSEI'], start_date, end_date )
df_nsei_index_returns=df_nsei_index_returns[1:]
df_nsei_index_returns.fillna(0, inplace=True)
df_nsei_returns=df_nsei_returns[1:]
df_nsei_returns.fillna(0, inplace=True)
index_returns=df_nsei.sum(axis=1)
index_returns=np.log(index_returns / index_returns.shift(1))
index_returns.dropna(inplace=True)
threshold= 1e-7
r=0.01
opt_value, opt_w, t, tracking_error= sparse_tracking(num_stocks, df_nsei_returns, df_nsei_index_returns['^NSEI'],  np.inf, threshold, r)
stocks_considered=np.array(nsei_stocks)[opt_w > threshold]
print('Found ', len(stocks_considered), ' stocks with weight bigger than threshold: ', stocks_considered)
weights_sparse_withoutMC=opt_w
print("Optimal weights: " ,opt_w)
print("Tracking error: ", (tracking_error)**0.5 )
print("Optimal Solution ", opt_value)

df_nsei_sparse=pd.DataFrame(df_nsei_returns.mul(opt_w, axis=1).sum(axis=1))
df_nsei_sparse.rename(columns={0:'NSEI_Replicated'}, inplace=True)
df_nsei_sparse['NSEI_orig']=df_nsei_index_returns
df_nsei_sparse.head()
df_nsei_sparse.plot()
plt.title("Replicated vs Original NSEI returns")
plt.show();

important_pesos=[abs(x) for x in opt_w if x>0.025]
important_labels=[stock for stock, x in zip(nsei_stocks, opt_w) if x>0.025]
peso_resto=sum([abs(x) for x in opt_w if x<0.025])
important_pesos.append(peso_resto)
important_labels.append('Others')
plt.pie(important_pesos, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of NSEI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

"""### 4th Method: Allocation using Non-Negative Least Squares (NNLS) Optimization

"""

df_nsei.fillna(0, inplace=True)
result = nnls(df_nsei_returns, list(df_nsei_index_returns['^NSEI']))
weights = result[0]
weights_nnls=weights
factor=result[1]
weights_plotting=[abs(x) for x in weights_nnls if x>0.025]
important_labels=[stock for stock, x in zip(nsei_stocks, weights_plotting) if x>0.025]
peso_resto=sum([abs(x) for x in weights_nnls if x<0.025])
weights_plotting.append(peso_resto)
important_labels.append('Others')
plt.pie(weights_plotting, labels=important_labels, autopct='%1.1f%%', wedgeprops=dict(width=0.3))
plt.title('Weights of NSEI stocks')
plt.legend(title='Stock index', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=2)
plt.show()

df_nsei_nnls=pd.DataFrame(df_nsei_returns.mul(weights, axis=1).sum(axis=1))
df_nsei_nnls.rename(columns={0:'NSEI_NNLS'}, inplace=True)
df_nsei_nnls['NSEI_orig']=df_nsei_index_returns
df_nsei_nnls.head()

"""## Comparison of several methods

### Histogram of daily returns
"""

df_nsei_comp=df_nsei_sparse
df_nsei_comp['NSEI_REP_PCA']=df_nsei_returns_replicated[['NSEI_REP_PCA']]
df_nsei_comp['NSEI_RISKFOLIO']=df_nsei_rp
df_nsei_comp['NSEI_NNLS']=df_nsei_nnls[['NSEI_NNLS']]

# Plot individual histograms for daily returns

for symbol in list(df_nsei_comp.columns):
    plt.figure(figsize=(6, 4))
    returns = df_nsei_comp[symbol].pct_change().dropna()
    sn.histplot(returns, bins=50, kde=True, label=symbol)
    plt.title('Histogram of Daily Returns')
    plt.xlabel('Daily Return')
    plt.ylabel('Frequency')
    plt.legend()
    plt.show()

"""### Difference of returns

"""

#Let's calculate difference between them
df_nsei_comp['Error_SPARSE_WITHOUT_MC']=df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_Replicated']
df_nsei_comp['Error_PCA']=df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_REP_PCA']
df_nsei_comp['Error_RISKFOLIO']=df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_RISKFOLIO']
df_nsei_comp['Error_NNLS']=df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_NNLS']

df_nsei_comp[['Error_SPARSE_WITHOUT_MC', 'Error_PCA', 'Error_RISKFOLIO', 'Error_NNLS']].plot()
plt.title("Replication method errors")
plt.show();

"""### Tracking errors

"""

df_tracking_error=pd.DataFrame()
df_tracking_error['SPARSE_WITHOUT_MC']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_Replicated']).apply(lambda x: np.sqrt(abs(x)/(df_nsei_comp['NSEI_orig'].shape[0]-1)))
df_tracking_error['PCA']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_REP_PCA']).apply(lambda x: np.sqrt(abs(x)/(df_nsei_comp['NSEI_orig'].shape[0]-1)))
df_tracking_error['RISKFOLIO']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_RISKFOLIO']).apply(lambda x: np.sqrt(abs(x)/(df_nsei_comp['NSEI_orig'].shape[0]-1)))
df_tracking_error['NNLS']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_NNLS']).apply(lambda x: np.sqrt(abs(x)/(df_nsei_comp['NSEI_orig'].shape[0]-1)))
df_tracking_error.plot()
plt.title("Tracking errors")
plt.show();

df_tracking_error.mean()

"""### Tracking differences

"""

df_tracking_difference=pd.DataFrame()
df_tracking_difference['SPARSE_WITHOUT_MC']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_Replicated'])
df_tracking_difference['PCA']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_REP_PCA'])
df_tracking_difference['RISKFOLIO']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_RISKFOLIO'])
df_tracking_difference['NNLS']=(df_nsei_comp['NSEI_orig']-df_nsei_comp['NSEI_NNLS'])
print(df_tracking_difference['SPARSE_WITHOUT_MC'].mean())
print(df_tracking_difference['PCA'].mean())
print(df_tracking_difference['RISKFOLIO'].mean())
print(df_tracking_difference['NNLS'].mean())
df_tracking_difference.plot()
plt.title("Tracking difference of different replication methods")
plt.show();

df_tracking_difference.mean()

"""
### Total Performance
"""

df_cummreturn=pd.DataFrame()
df_cummreturn['PCA']=(1 + df_nsei_comp['NSEI_REP_PCA'].fillna(0)).cumprod() - 1
df_cummreturn['RISKFOLIO']=(1 + df_nsei_comp['NSEI_RISKFOLIO'].fillna(0)).cumprod() - 1
df_cummreturn['SPARSE_WITHOUT_MC']=(1 + df_nsei_comp['NSEI_Replicated'].fillna(0)).cumprod() - 1
df_cummreturn['NNLS']=(1 + df_nsei_comp['NSEI_NNLS'].fillna(0)).cumprod() - 1
df_cummreturn['^NSEI']=(1 + df_nsei_comp['NSEI_orig'].fillna(0)).cumprod() - 1
df_cummreturn.plot()
plt.title("Total Perfomance: Cummulative returns NSEI index")
plt.show();

df_cummreturn.mean()

"""### Sharpe index

"""

# Suppose risk free rate for example 0.5%
risk_free_rate=0.5/100
excess_return=pd.DataFrame()
excess_return['^NSEI']=df_nsei_comp['NSEI_orig']-risk_free_rate
excess_return['PCA']=df_nsei_comp['NSEI_REP_PCA']-risk_free_rate
excess_return['RISKFOLIO']=df_nsei_comp['NSEI_RISKFOLIO']-risk_free_rate
excess_return['SPARSE_WITHOUT_MC']=df_nsei_comp['NSEI_Replicated']-risk_free_rate
excess_return['NNLS']=df_nsei_comp['NSEI_NNLS']-risk_free_rate
sharpe_ratio_orig=np.mean(excess_return['^NSEI'])/np.std(excess_return['^NSEI'])
sharpe_ratio_pca=np.mean(excess_return['PCA'])/np.std(excess_return['PCA'])
sharpe_ratio_riskfolio=np.mean(excess_return['RISKFOLIO'])/np.std(excess_return['RISKFOLIO'])
sharpe_ratio_sparse_without_mc=np.mean(excess_return['SPARSE_WITHOUT_MC'])/np.std(excess_return['SPARSE_WITHOUT_MC'])
sharpe_ratio_nnls=np.mean(excess_return['NNLS'])/np.std(excess_return['NNLS'])
print('Sharpe Ratios: ')
print('Original index: ', sharpe_ratio_orig)
print('Replication with PCA index: ', sharpe_ratio_pca)
print('Replication with Riskfolio index: ', sharpe_ratio_riskfolio)
print('Replication with Sparse without MonteCarlo index: ', sharpe_ratio_sparse_without_mc)
print('Replication with NNLS index: ', sharpe_ratio_nnls)
excess_return.plot()
plt.title("Excess of returns in replication NSEI index")
plt.show();

"""### Comparison of prices: Costs

"""

df_prices=pd.DataFrame()
df_prices['SPARSE_WITHOUT_MC']=pd.DataFrame(df_nsei.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_prices['PCA']=pd.DataFrame(df_nsei.mul(weights_pca, axis=1).sum(axis=1))
df_prices['RISKFOLIO']=pd.DataFrame(df_nsei.mul(weights_riskfolio, axis=1).sum(axis=1))
df_prices['NNLS']=pd.DataFrame(df_nsei.mul(weights_nnls, axis=1).sum(axis=1))
df_prices['^NSEI']=df_nsei_index

df_prices.head()

df_prices.describe()

df_prices[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Prices of replication NSEI index")
plt.show();

df_prices.mean()

"""### Liquidity"""

df_volumen=extract_liq(nsei_stocks, start_date, end_date)
df_volumen_index=yf.download('^NSEI', start=start_date, end=end_date)['Volume']
df_liq=pd.DataFrame()
df_liq['SPARSE_WITHOUT_MC']=pd.DataFrame(df_volumen.mul(weights_sparse_withoutMC, axis=1).sum(axis=1))
df_liq['PCA']=pd.DataFrame(df_volumen.mul(weights_pca, axis=1).sum(axis=1))
df_liq['RISKFOLIO']=pd.DataFrame(df_volumen.mul(weights_riskfolio, axis=1).sum(axis=1))
df_liq['NNLS']=pd.DataFrame(df_volumen.mul(weights_nnls, axis=1).sum(axis=1))
df_liq['^NSEI']=df_volumen_index
df_liq.head()

df_liq[['SPARSE_WITHOUT_MC', 'PCA', 'RISKFOLIO', 'NNLS']].plot()
plt.title("Liquidity of NSEI replicas")
plt.show();

df_liq.mean()



"""# Converting into pdf"""

!jupyter nbconvert --to html /content/MscFE_Capstone_Project_alternative_methods.ipynb

!pip install pandoc

!jupyter nbconvert --to pdf /content/MscFE_Capstone_Project_alternative_methods.ipynb

pip install  pandoc

